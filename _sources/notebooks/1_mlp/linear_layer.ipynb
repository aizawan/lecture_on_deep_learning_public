{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 線形層\n",
    "\n",
    "ニューラルネットワークの文脈において，**線形層（linear layer）** または **全結合層（fully-connected layer）** は入力行列 $\\boldsymbol{x} \\in \\mathbb{R}^{B \\times D_{in}}$ に対して，重み行列 $\\boldsymbol{W} \\in \\mathbb{R}^{D_{out} \\times D_{in}}$ との行列計算を行い，バイアス項 $\\boldsymbol{b} \\in \\mathbb{R}^{D_{out}}$ を足し合わせる以下の線形変換を行う層である．\n",
    "\n",
    "$$\n",
    "\\boldsymbol{h} = \\boldsymbol{x} \\boldsymbol{W}^\\top + \\boldsymbol{b}\n",
    "$$\n",
    "\n",
    "ここで，$D_{in}$ と $D_{out}$ は入出力の **特徴次元** とも呼ばれ，線形層の出力 $\\boldsymbol{h}$ は行列計算からも明らかであるように $B \\times D_{out}$ 次元である．$B$ は一般的にミニバッチサイズを示す．線形層から構成されるMulti-Layer Perceptron(MLP)では，この重みとバイアスを誤差逆伝播法から学習する．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 線形層の詳細\n",
    "\n",
    "行列計算の復習のために，線形層の詳細な計算を確認する．入力行列，重み行列，バイアス項を要素単位で表記すると次のようになる．\n",
    "\n",
    "$$\n",
    "\\boldsymbol{x} = \\left[\\begin{array}{cccc}\n",
    "x_{11} & x_{12} & \\cdots & x_{1D_{in}} \\\\\n",
    "x_{21} & x_{22} & \\cdots & x_{2D_{in}} \\\\\n",
    "\\vdots & \\vdots & & \\vdots \\\\\n",
    "x_{B1} & x_{B2} & \\cdots & x_{BD_{in}}\n",
    "\\end{array}\\right], \n",
    "\\boldsymbol{W}^\\top = \\left[\\begin{array}{cccc}\n",
    "w_{11} & w_{21} & \\cdots & w_{D_{out}1} \\\\\n",
    "w_{12} & w_{22} & \\cdots & w_{D_{out}2} \\\\\n",
    "\\vdots & \\vdots & & \\vdots \\\\\n",
    "w_{1D_{in}} & w_{2D_{in}} & \\cdots & w_{D_{out}D_{in}}\n",
    "\\end{array}\\right]\n",
    "\\boldsymbol{b} = \\left[\\begin{array}{c}\n",
    "b_1 \\\\\n",
    "b_2 \\\\\n",
    "\\vdots \\\\\n",
    "b_{D_{out}}\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "これらを基に線形変換を書くと\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\boldsymbol{h} &= \\left[\\begin{array}{cccc}\n",
    "\\textcolor{#0099ff}{x_{11}} & \\textcolor{#0099ff}{x_{12}} & \\cdots & \\textcolor{#0099ff}{x_{1D_{in}}} \\\\\n",
    "x_{21} & x_{22} & \\cdots & x_{2D_{in}} \\\\\n",
    "\\vdots & \\vdots & & \\vdots \\\\\n",
    "x_{B1} & x_{B2} & \\cdots & x_{BD_{in}}\n",
    "\\end{array}\\right] \\left[\\begin{array}{cccc}\n",
    "\\textcolor{#0099ff}{w_{11}} & w_{21} & \\cdots & w_{D_{out}1} \\\\\n",
    "\\textcolor{#0099ff}{w_{12}} & w_{22} & \\cdots & w_{D_{out}2} \\\\\n",
    "\\vdots & \\vdots & & \\vdots \\\\\n",
    "\\textcolor{#0099ff}{w_{1D_{in}}} & w_{2D_{in}} & \\cdots & w_{D_{out}D_{in}}\n",
    "\\end{array}\\right] + \n",
    "\\left[\\begin{array}{c}\n",
    "\\textcolor{#0099ff}{b_1} \\\\\n",
    "b_2 \\\\\n",
    "\\vdots \\\\\n",
    "b_{D_{out}}\n",
    "\\end{array}\\right] \\\\\n",
    "&= \\left[\\begin{array}{cccc}\n",
    "\\textcolor{#cc0000}{h_{11}} & h_{12} & \\cdots & h_{1D_{out}} \\\\\n",
    "h_{21} & h_{22} & \\cdots & h_{2D_{out}} \\\\\n",
    "\\vdots & \\vdots & & \\vdots \\\\\n",
    "h_{B1} & h_{B2} & \\cdots & h_{BD_{out}}\n",
    "\\end{array}\\right]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "となる．色をつけた出力行列 $\\boldsymbol{h}$ の要素 $h_{11}$ は，入力行列 $\\boldsymbol{x}$ の第1行と，転置された重み行列 $\\boldsymbol{W}^\\top$ の第1列の各要素の積の総和に，バイアス項 $b_1$ を加えたものとして表され，\n",
    "\n",
    "$$\n",
    "h_{11} = x_{11} w_{11} + x_{12} w_{21} + x_{13} w_{31} + \\cdots + x_{1D_{in}} w_{D_{in}1} + b_1\n",
    "$$\n",
    "\n",
    "と書ける．これは一つの入力ベクトル $\\boldsymbol{x}_1$ と対応する結合重み $\\boldsymbol{w}_1$ との内積とも解釈できる．\n",
    "\n",
    "$$\n",
    "h_{11} = (\\boldsymbol{x}_1, \\boldsymbol{w}_1) + b_1\n",
    "$$\n",
    "\n",
    "これをすべてのサンプルと全次元にわたって計算するのが線形層である．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 線形層の実装\n",
    "\n",
    "PyTorchを使って線形層の計算を確認する．PyTorchの線形層 [`torch.nn.Linear`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear) は\n",
    "\n",
    "```\n",
    "torch.nn.Linear(in_features, out_features, bias=True, device=None, dtype=None)\n",
    "```\n",
    "\n",
    "で作成することができる．ここで，`in_features` が入力の特徴次元 $D_{in}$ であり，`out_features` が出力の特徴次元 $D_{out}$ である．`bias`の引数でバイアスの有無を選択できる．\n",
    "\n",
    "以下がランダムに初期化した入力を線形層に適用したときのプログラム例である．次元と引数を確認されたい．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "x = torch.randn(4, 3) # print(x.shape) -> torch.Size(128,20])\n",
    "fc_layer = nn.Linear(3, 2) \n",
    "h = fc_layer(x) # print(h.shape) -> torch.Size([20,10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "線形層の重みとバイアスは以下のように出力できる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_layer.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "行列の計算からもわかるように，適切に入出力の次元が設定できないと行列計算時にエラーが発生する．`.shape`を使って計算対象の形状を適宜確認すると良い．"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
