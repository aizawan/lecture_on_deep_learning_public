{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習と評価\n",
    "\n",
    "このノートブックでは，構築されたモデルをデータセットを用いて学習する処理を実装する．モデルの学習ループ（1 iteration）は一般的に次のような手順となる．\n",
    "\n",
    "1. ミニバッチの作成\n",
    "2. 順伝播\n",
    "3. 損失の計算\n",
    "4. 勾配計算とパラメータの更新\n",
    "5. 学習結果の保存\n",
    "\n",
    "ここでは，このループを説明するために，moon_datasetを使って分類問題を解く．まずはミニバッチの作成のためにこのデータセットの生成と可視化を行おう．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データセットの作成\n",
    "\n",
    "[moons dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html)は線形分離が不可能なシンプルなトイデータセットであり，クラスタリングや二値分類のアルゴリズムの検証に利用できる．可視化をすると半月が対称に重なるような分布である．\n",
    "\n",
    "moon_datasetは`scikit-learn`から容易に利用できる．次のコードで読み込みができる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=500, noise=0.1, random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`X` と `y` は"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "返り値の `X` と `y` はそれぞれモデルの入出力に対応する入力データと正解値となっている．これらの形状（`.shape`）を確認する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X: ', X.shape)\n",
    "print('y: ', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "形状（`.shape`）からも分かるように入力データは(サンプル数，入力次元数)となっている．つまりは，以下のようにすると，$i$番目のサンプルの値を取得できる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 10\n",
    "print(X[i], y[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最初の数サンプルをスライシングで出力してみる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X = ', X[:3])\n",
    "print('y = ', y[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`y` はそのデータに対応する正解値（ここでは，分類問題なのでクラスラベル）が格納されている．moon_datasetは2クラスなので，`np.unique` 関数などで配列の中に含まれるユニークな要素を出力してみると，`0` と `1` の二つの値が入っていることがわかる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "続いて，これらのデータとラベルからmoon_datasetを可視化してみる．入力特徴次元は2次元なのでそれぞれx軸y軸として散布図を描く．このとき，点の色をラベル値に応じてつける．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=10)\n",
    "plt.title(\"Moons Data\")\n",
    "plt.grid(True)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データの分布が可視化できたと思う．データの可視化はこのようなトイデータセットだけでなく画像・言語・信号であってもそのデータの特性を理解する上で非常に重要である（例えば，そもそも線形分離可能なデータであったならば，深層学習を利用する必要がない）．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "続いて，モデルの学習と評価のために，データセットを **学習（train）**，**検証（validation）**，**評価（test）** 用に分割する．これらの役割は\n",
    "\n",
    "- 学習データ：モデルのパラメータを学習するため\n",
    "- 評価データ：未知のデータに対するモデルの性能を評価するため\n",
    "- 検証データ：学習可能なパラメータ以外のパラメータ（例えば，層数や隠れ層の次元）を調整するため\n",
    "\n",
    "である．インターネット上の説明には学習と評価のみの解説や実装も多いが，検証データがないと，本来未知であるべきの評価データを使ってモデルのパラメータを探索することとなる．これは未知のデータに対する評価ではなくなり，正当なモデルの評価ができているとは言えない実験となる．\n",
    "\n",
    "この分割自体は次のように，`scikit-learn` の関数を用いれば容易に実装できる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=1234)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=1234)\n",
    "\n",
    "print(f\"train data: {len(X_train)}\")\n",
    "print(f\"validation data: {len(X_valid)}\")\n",
    "print(f\"test data: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで，`random_state`を指定したが，これは分割のランダム性を固定する処理である．固定しないと毎回結果が変わるので再現性が担保できない．可能な限り，再現性を意識した実装をしよう．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データローダーの作成\n",
    "確率的勾配降下法でモデルを学習するために，作成したデータセットからランダムにデータを取り出して，ミニバッチ（minibatch）と呼ばれるサブセットを作成する必要がある．PyTorchではこの処理をデータローダー（`DataLoader`）として実装する．この機能は，今回のような数値データだけでなく，画像，音声，言語など様々なデータを扱うことができ，また並列化も内部でサポートしている．\n",
    "\n",
    "以下のセルでデータローダーの作成を行う．まずは`numpy`形式のデータセットを`torch.Tensor`へ`torch.from_numpy`で変換する．このとき，入力データは `torch.float32`，ラベル値は整数値として `torch.long` で型を定義する必要がある．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "_X_train = torch.from_numpy(X_train).to(torch.float32)\n",
    "_y_train = torch.from_numpy(y_train).to(torch.long)\n",
    "_X_valid = torch.from_numpy(X_valid).to(torch.float32)\n",
    "_y_valid = torch.from_numpy(y_valid).to(torch.long)\n",
    "_X_test = torch.from_numpy(X_test).to(torch.float32)\n",
    "_y_test = torch.from_numpy(y_test).to(torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データ型が適切に変換されていることがわかる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(X_train), type(y_train))\n",
    "print(type(_X_train), type(_y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "続いて，変換されたデータをPyTorchのデータセットクラスの一種である`TensorDataset`へ変換する．このデータセットクラスはデータからサンプル一つだけを取り出し，データローダーへ渡すメソッドが定義されている．`TensorDataset`の作成は次のように，入力データと正解値をそれぞれ引数として渡せば良い．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "train_dataset = TensorDataset(_X_train, _y_train)\n",
    "valid_dataset = TensorDataset(_X_valid, _y_valid)\n",
    "test_dataset = TensorDataset(_X_test, _y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次のように`print`してもわかるように，データセットクラスとなっている．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "補足であるが，データセットのサイズや$i$番目のサンプルは次のようにデータセットから取得できる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_dataset))\n",
    "\n",
    "i = 10\n",
    "x_i, y_i = train_dataset[i]\n",
    "print(x_i, y_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最後に，データローダーを作成する．並列化や複数GPUでの学習の場合は引数を加える必要があるが，ここではバッチサイズのみを指定する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=10)\n",
    "test_loader = DataLoader(test_dataset, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データローダーは`for`文を使って以下のようにミニバッチを作成してくれる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    x, y = batch\n",
    "    print(x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの学習ループの作成\n",
    "\n",
    "確率的勾配降下法でも説明したように，ニューラルネットワークのパラメータの**一回**の更新は(1)`optimizer.zero_grad()` による勾配の初期化，(2)順伝播の実行と損失の計算，(3)`.backward`による勾配計算，(4)`optimizer.step()`によるパラメータの更新\n",
    "という一連の処理からなる．そして，この処理を1回実行することを，**1 iteration** といった．\n",
    "\n",
    "モデルの構築，損失関数とオプティマイザの設定を行い，この1 iterationを実装しよう．今回の例では，2次元の入力を受け取り，クラス0か1を出力する．したがって，交差エントロピー関数を損失関数として設定する．\n",
    "\n",
    "**補足：** 2クラス分類なので出力次元を1としてSigmoid関数と二値エントロピー関数を損失としても良い．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layer_in = nn.Linear(input_dim, hidden_dim)\n",
    "        self.layer_hidden = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.layer_out = nn.Linear(hidden_dim, output_dim)\n",
    "        self.act = nn.ReLU()\n",
    "        self.act_out = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = x\n",
    "        h = self.layer_in(h)\n",
    "        h = self.act(h)\n",
    "        h = self.layer_hidden(h)\n",
    "        h = self.act(h)\n",
    "        h = self.layer_out(h)\n",
    "        h = self.act_out(h)\n",
    "        return h\n",
    "    \n",
    "model = MLP(input_dim=2, hidden_dim=32, output_dim=2)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これらをベースにDataLoaderの`for`ループ内に書くと次のようになる．\n",
    "\n",
    "```python\n",
    "for batch in train_loader:\n",
    "    x, y = batch       \n",
    "    optimizer.zero_grad()\n",
    "    output = model(x)\n",
    "    loss = loss_function(output, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "```\n",
    "\n",
    "このループは データ数 / ミニバッチ数 回だけ行われる．この回数は `len(train_loader)` でも取得できる．そして，このループが終了した，つまり，データセットに含まれるすべてのデータを使ってモデルを学習したとき，その1回の回数を **1 epoch** と言う．iterationとepochは区別して表記しよう．\n",
    "\n",
    "このループ処理，つまり，1 epochの処理は，$N$ epoch回学習を回すので再利用しやすいよう関数化しておこう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, loss_function, optimizer):\n",
    "    model.train()\n",
    "    train_loss, train_acc = [], []\n",
    "    for batch in loader:\n",
    "        x, y = batch\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        loss = loss_function(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "        acc = (output.max(1)[1] == y).float().mean()\n",
    "        train_loss.append(loss.item())\n",
    "        train_acc.append(acc.item())\n",
    "    return np.mean(train_loss), np.mean(train_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで，`model.train()` はモデルを学習モードにする処理である．ニューラルネットワークには学習時と推論時で処理が異なる層がある．それを制御するための呼び出しである．\n",
    "\n",
    "加えて，各iterationの損失と分類精度を記録するリストを`train_loss`と`train_acc`として作成してこれらの値を記録している．後述するが損失と精度の推移はうまく学習が進んでいるかを確認するための重要な情報である．毎回記録しよう．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "では，これを $N=100$ epochだけ学習させる．関数化しているので次のように実装すれば良い．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "train_loss, train_acc = [], []\n",
    "for epoch in range(1, epochs+1):\n",
    "    loss, acc = train_one_epoch(model, train_loader, loss_function, optimizer)\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}/{epochs}: loss - {loss:.4f}, acc - {acc:.4f}')\n",
    "    train_loss.append(loss)\n",
    "    train_acc.append(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実行してみると，100 epoch分の学習が実行され，各epochの平均の損失が徐々に減少し，精度が徐々に増加していることがわかる．これはつまり正解ラベルに近い学習ができていることを意味している．上記のコードではこの学習の推移を`train_loss`と`train_acc`に保存している．\n",
    "\n",
    "この損失が十分に下がり切るまで学習を行えば良い．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 精度の計算の補足\n",
    "\n",
    "精度の計算 `acc = (output.max(1)[1] == y).float().mean()` について補足しておく．\n",
    "モデルからの予測結果の出力 `output` は次のように各データに対して，クラス数分だけ出力されている．つまり，ミニバッチ数を行，クラス数を列とした行列になっている．そして，正解ラベルはミニバッチ数次元だけのベクトルであり，各要素には正解ラベルのインデックスが格納されている．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = torch.tensor([\n",
    "    [0.2, 0.5, 0.3],  # サンプル1の予測スコア\n",
    "    [0.1, 0.1, 0.8],  # サンプル2の予測スコア\n",
    "    [0.1, 0.3, 0.6],  # サンプル3の予測スコア\n",
    "    [0.3, 0.4, 0.3]   # サンプル4の予測スコア\n",
    "])\n",
    "\n",
    "y = torch.tensor([1, 0, 2, 1])  # 正解ラベル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "得られた処理に対して `output.max(1)[1]` を計算している．`output.max(1)`では，各サンプルに対して最も大きな値（高い予測スコア）を持つクラスのインデックスとその値を計算して返している．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values, indices = output.max(1)\n",
    "print('value:', values)\n",
    "print('indices:', indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "そのため，`output.max(1)[1]` は最大値を持つクラスのインデックスを取得していることとなる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.max(1)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "そして，次の `output.max(1)[1] == y` ではこのインデックスを正解ラベルのインデックスと比較して，同じ場合は `True`，違う場合は `False` として，データ数分だけでの要素を持つTensorを返す．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('indices:', output.max(1)[1])\n",
    "print('y:', y)\n",
    "\n",
    "print(output.max(1)[1] == y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この情報からミニバッチあたり何サンプルが正解したかを計算できる．そのために，`True`を1.0，`False`を0.0とする数値のベクトルに変換して，平均を取れば良い．それが精度計算で行なっている処理である．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(output.max(1)[1] == y).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 評価\n",
    "\n",
    "学習の損失と精度に関しては正解ラベルを使ってパラメータを更新するので，適切なモデルとハイパーパラメータを設定し，バグがなければ基本的には損失は更新を繰り返すたびに減少し，精度も高くなっていく．しかしながら，本来評価したいのは，**学習に用いていないデータ** の予測精度である．そこで，データセットの作成時に分割した `test_dataset` を使ってモデルを評価しよう．\n",
    "\n",
    "評価は `train_loader` の代わりに `test_loader` を使えば良い．関数化した `train_one_epoch` をベースに評価用の関数 `test` を実装しよう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, loader, loss_function):\n",
    "    model.eval()\n",
    "    test_loss, test_acc = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            x, y = batch\n",
    "            output = model(x)\n",
    "            loss = loss_function(output, y)\n",
    "      \n",
    "            acc = (output.max(1)[1] == y).float().mean()\n",
    "            test_loss.append(loss.item())\n",
    "            test_acc.append(acc.item())\n",
    "    return np.mean(test_loss), np.mean(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "評価時はパラメータの更新が必要ないので，`optimizer` を渡さず，更新処理に関連する勾配計算も行わない．また，評価用モードで実行したいので `model.eval()` を呼び出している．\n",
    "\n",
    "以下のように学習が完了したモデルと`test_loader`を渡してテストデータに対する分類性能を確認する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = test(model, test_loader, loss_function)\n",
    "print('test_loss = ', test_loss)\n",
    "print('test_acc = ', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 検証\n",
    "\n",
    "検証データ（`val_dataset`）は学習率や層数などのハイパーパラメータを調整するために使用する．評価で用いた`test`関数に`test_loader`ではなく，`val_loader`を渡して，学習時に含まれていないデータでの学習の過程を記録しておこう．そのために，学習で作成した学習ループを次のように修正する．\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(input_dim=2, hidden_dim=32, output_dim=2)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "epochs = 100\n",
    "train_loss, train_acc = [], []\n",
    "valid_loss, valid_acc = [], []\n",
    "for epoch in range(1, epochs+1):\n",
    "    loss, acc = train_one_epoch(model, train_loader, loss_function, optimizer)\n",
    "    train_loss.append(loss)\n",
    "    train_acc.append(acc)\n",
    "    \n",
    "    loss, acc = test(model, valid_loader, loss_function)\n",
    "    valid_loss.append(loss)\n",
    "    valid_acc.append(acc)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}/{epochs}')\n",
    "        print(f'train_loss - {loss:.4f}, train_acc - {acc:.4f}')\n",
    "        print(f'valid_loss - {loss:.4f}, valid_acc - {acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これで，各epochで学習と検証を行うことができる．一般的に，評価するモデルは，$N$ epoch学習が完了した時点でのモデルを利用する，もしくは**早期終了（early stopping）** と呼ばれる検証データを精度の推移から評価モデルを決定する．評価（テスト）は **未知のデータ** である必要があるので，`test_dataset` を使ってハイパーパラメータ等の調整をしてはいけない．代わりに `valid_dataset` を利用しよう．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの保存\n",
    "\n",
    "学習と評価が完了したモデルを保存する．保存の際には，PyTorchが提供する `state_dict` を用いて，モデルの全てのパラメータを辞書形式で取得する．`state_dict`は，PyTorchの各レイヤーやオプティマイザの内部状態が含まれているが，モデル構造そのものは含まず，パラメータの値のみを保存することに注意されたい．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('output', exist_ok=True)\n",
    "\n",
    "save_path = 'output/model.pth'\n",
    "torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "保存されたモデルパラメータを再度読み込むには，保存した`.model.pth`は`torch.load`関数を用いて`state_dict`をロードし，`load_state_dict`メソッドを使用して保存されたパラメータを読み込む必要がある．\n",
    "\n",
    "注意点として，`state_dict`ではモデルの構造は保存されていないので，読み込むモデルと保存したモデルの構造が異なる場合はエラーが生ずる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = MLP(input_dim=2, hidden_dim=32, output_dim=2)\n",
    "loaded_model.load_state_dict(torch.load(save_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再度，評価を行うと先ほどと同じ結果が得られていることがわかる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = test(loaded_model, test_loader, loss_function)\n",
    "print('test_loss = ', test_loss)\n",
    "print('test_acc = ', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習結果の保存\n",
    "\n",
    "学習の過程を保存した損失と精度を後から読み出せるように保存しておこう．`pandas`，`csv`，`pickle`を使って保存することもできるが，ここではnumpyの`save_txt`を使って保存する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('output/train_loss.txt', train_loss)\n",
    "np.savetxt('output/train_acc.txt', train_acc)\n",
    "\n",
    "np.savetxt('output/valid_loss.txt', valid_loss)\n",
    "np.savetxt('output/valid_acc.txt', valid_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
