{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習率の調整\n",
    "\n",
    "学習率は，パラメータの更新幅を制御するための重要なハイパーパラメータであり，オプティマイザの中で動的に学習率を調整する手法（Adamなど）も提案されているが，適切に設定しないと学習の収束が遅くなったり，局所解や発散するような解に到達してしまう．一般的に，学習初期では高めの学習率を，学習後期では学習率を減少させることが多い．\n",
    "\n",
    "**学習率の調整（Learning Rate Scheduling）** は，モデルの学習過程で学習率を特定のスケジュールに沿って動的に変更することで，学習効率や汎化性能を改善するためのテクニックである．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorchでは，`torch.optimi.lr_scheduler` に様々な学習率のスケジューラが実装されており，簡単に利用できる．また学習率は次のようにEpochの終わりに更新されることが多い．\n",
    "\n",
    "```\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "for epoch in range(20):\n",
    "    for input, target in dataset:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    scheduler.step()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "よく利用される標準的なスケジューラは\n",
    "\n",
    "- 一定のEpoch数ごとに学習率を減少させる `StepLR`\n",
    "- 指数関数に基づき学習率を減衰させる `ExponentialLR`\n",
    "- $\\cos$ 関数に基づいて学習率を上下させる `CosineAnnealingLR`\n",
    "\n",
    "である．学習するタスクやモデルに応じてスケジューラもまた適切に設定する必要がある．\n",
    "\n",
    "学習率の調整の過程を可視化する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import StepLR, LinearLR, ExponentialLR, CosineAnnealingLR, CyclicLR\n",
    "\n",
    "initial_lr = 0.1\n",
    "epochs = 100\n",
    "\n",
    "lr_histories = {\n",
    "    \"Step Decay\": [],\n",
    "    \"Exponential Decay\": [],\n",
    "    \"Cosine Annealing\": [],\n",
    "}\n",
    "\n",
    "for name in lr_histories.keys():\n",
    "    model = torch.nn.Linear(1, 1)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=initial_lr)\n",
    "    \n",
    "    if name == \"Step Decay\":\n",
    "        scheduler = StepLR(optimizer, step_size=25, gamma=0.5)\n",
    "    elif name == \"Exponential Decay\":\n",
    "        scheduler = ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif name == \"Cosine Annealing\":\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=20, eta_min=0)\n",
    "  \n",
    "    for epoch in range(epochs):\n",
    "        lr_histories[name].append(optimizer.param_groups[0][\"lr\"])\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "for name, lr_history in lr_histories.items():\n",
    "    plt.plot(lr_history, label=name)\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.title(\"Learning Rate Scheduling in PyTorch\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "他にもスケジューリングの方法は提案されており，[PyTorchのリファレンス](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate)より確認できるので確認されたい．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
