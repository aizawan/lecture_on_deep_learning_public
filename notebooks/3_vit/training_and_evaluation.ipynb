{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習と評価\n",
    "\n",
    "このノートブックでは，Vision Transformerの学習と評価を行うためのパイプラインを実装する．今回は，画像を直接入力できるように，かつ，ヘッドからの予測結果がCNNの形状と同じになるようにViTを実装したので，モデルの定義以外はCNNのコードをすべて流用できる．\n",
    "\n",
    "そのため，CNNではMNISTを使ったが，今回はカラー画像のCIFAR10を利用して学習させてみよう．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データセットの準備\n",
    "\n",
    "CIFAR10は10クラスからなる一般物体の画像認識用データセットであり，各データはRGBの3チャネル持つ．CIFAR10もMNISTと同じように `torchvision` でサポートされており，次のように簡単に読み込むことができる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "dataset = datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, \n",
    "    transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIFAR10も初回実行時は`root`にデータをダウンロードするため時間がかかる．正しくダウンロードできたならば，`train=True` としたので学習データセットが利用可能になる．\n",
    "\n",
    "サンプル数と画像サイズをチェックする．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('len(dataset):', len(dataset))\n",
    "\n",
    "i = 0\n",
    "x, y = dataset[i]\n",
    "\n",
    "print('x.shape:', x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "チャネル数が3チャネル，画像サイズが $32 \\times 32$ であることが確認できる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データの最小値・最大値は"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('x.min():', x.min())\n",
    "print('x.max():', x.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "となり，`ToTensor()` を渡しているので0から1の間に正規化されている．ここでは，さらに **標準化（Standardization）** を行おう．標準化はデータの平均を0，分散を1とする手法であり，ニューラルネットの学習においても性能改善のための重要なテクニックである．\n",
    "\n",
    "各チャネルの平均と分散を計算しよう．内方表記を利用して学習データセットに含まれるデータを一度一つのテンソル化し，バッチ，幅，高さ方向に対して平均と標準偏差を次のように計算する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "data = torch.cat([d[0] for d in torch.utils.data.DataLoader(dataset)])\n",
    "mean = data.mean(dim=[0, 2, 3])\n",
    "std = data.std(dim=[0, 2, 3])\n",
    "\n",
    "print('mean:', mean)\n",
    "print('std:', std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "計算できたので，これを使って標準化を行う`transform`を作成する．今回は学習用にデータ拡張を施した`transform`も作成しよう．またオリジナルのCIFAR10の解像度は確認した通り $32 \\times 32$ であるが，ViTへの入力（パッチ化）を考慮して，$96 \\times 96$ とする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 96\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(img_size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(img_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再度データセットを読み込んで学習・検証・評価データセットを作成する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True,\n",
    "    transform=train_transform)\n",
    "\n",
    "train_size = int(len(train_dataset) * 0.9)\n",
    "valid_size = int(len(train_dataset) * 0.1)\n",
    "train_dataset, valid_dataset = \\\n",
    "    torch.utils.data.random_split(\n",
    "        train_dataset, [train_size, valid_size])\n",
    "        \n",
    "test_dataset = datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True,\n",
    "    transform=test_transform)\n",
    "\n",
    "print(f'train data: {len(train_dataset)}')\n",
    "print(f'validation data: {len(valid_dataset)}')\n",
    "print(f'test data: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データローダーを作成する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=100)\n",
    "test_loader = DataLoader(test_dataset, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ミニバッチを次のように取り出して，画像データを可視化しよう．ここで利用した`torchvision.utils.make_grid` 関数はミニバッチの画像データの可視化の際に非常に便利である．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x, y = next(iter(train_loader))\n",
    "\n",
    "mean_ = torch.tensor(mean.numpy()).view(1, 3, 1, 1)\n",
    "std_ = torch.tensor(std.numpy()).view(1, 3, 1, 1)\n",
    "x = x * std_ + mean_\n",
    "\n",
    "img = make_grid(x[:25], nrow=5)\n",
    "plt.imshow(img.permute(1,2,0).numpy())\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下の処理\n",
    "\n",
    "```\n",
    "mean_ = torch.tensor(mean.numpy()).view(1, 3, 1, 1)\n",
    "std_ = torch.tensor(std.numpy()).view(1, 3, 1, 1)\n",
    "x = x * std_ + mean_\n",
    "```\n",
    "\n",
    "は標準化の逆を行い，元の範囲に戻している．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの定義\n",
    "\n",
    "データローダーが構築できたのでViTの構築を行う．行数が長くなるが，ViTのノートブックから必要な処理をコピペしている．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim, hidden_dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, dim)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=4, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        self.proj_q = nn.Linear(dim, dim, bias=False)\n",
    "        self.proj_k = nn.Linear(dim, dim, bias=False)\n",
    "        self.proj_v = nn.Linear(dim, dim, bias=False)\n",
    "        self.proj = nn.Linear(dim, dim, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        bs, num_tokens, dim = x.shape\n",
    "        \n",
    "        q = self.proj_q(x)\n",
    "        k = self.proj_q(x)\n",
    "        v = self.proj_q(x)\n",
    "        \n",
    "        q = q.reshape(bs, num_tokens, self.num_heads, self.head_dim)\n",
    "        k = k.reshape(bs, num_tokens, self.num_heads, self.head_dim)\n",
    "        v = v.reshape(bs, num_tokens, self.num_heads, self.head_dim)\n",
    "    \n",
    "        attn_weight = q @ k.transpose(-2, -1) * dim ** -0.5\n",
    "        attn_weight = F.softmax(attn_weight, dim=-1)\n",
    "        attn_weight = self.dropout(attn_weight)\n",
    "        x = attn_weight @ v\n",
    "        \n",
    "        x = x.transpose(1, 2).reshape(bs, num_tokens, dim)\n",
    "        x = self.proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.attn = Attention(dim, num_heads, dropout)\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp = MLP(dim, dim, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.norm1(x)\n",
    "        h = self.attn(h)\n",
    "        h = x + h\n",
    "        h = self.norm2(h)\n",
    "        h = self.mlp(h)\n",
    "        h = x + h\n",
    "        return h\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fc = nn.Linear(dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=384):\n",
    "        super().__init__()\n",
    "        self.num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
    "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, img_size, patch_size, in_channels, num_classes, embed_dim, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbed(img_size, patch_size, in_channels, embed_dim)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        num_patches = self.patch_embed.num_patches + 1\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n",
    "\n",
    "        self.block1 = Block(embed_dim, num_heads, dropout)\n",
    "        self.block2 = Block(embed_dim, num_heads, dropout)\n",
    "        self.block3 = Block(embed_dim, num_heads, dropout)\n",
    "\n",
    "        self.head = Head(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        cls_tokens = self.cls_token.expand(x.shape[0], -1, -1)\n",
    "        x = torch.torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "\n",
    "        x = self.head(x[:,0])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "インスタンス化を行う．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViT(img_size, 16, 3, 10, 256, 4, 0.1)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 損失関数とオプティマイザの設定・GPUへの移動\n",
    "\n",
    "続いて，損失関数，オプティマイザを設定する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPUへの移動も行う．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習・検証ループ\n",
    "\n",
    "こちらもCNNからの流用でコピペして実行する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def train_one_epoch(model, loader, loss_function, optimizer, device):\n",
    "    model.train()\n",
    "    train_loss, train_acc = [], []\n",
    "    for batch in loader:\n",
    "        x, y = batch\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        loss = loss_function(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "        acc = (output.max(1)[1] == y).float().mean()\n",
    "        train_loss.append(loss.item())\n",
    "        train_acc.append(acc.item())\n",
    "    return np.mean(train_loss), np.mean(train_acc)\n",
    "\n",
    "def test(model, loader, loss_function, device):\n",
    "    model.eval()\n",
    "    test_loss, test_acc = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            x, y = batch\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            output = model(x)\n",
    "            loss = loss_function(output, y)\n",
    "      \n",
    "            acc = (output.max(1)[1] == y).float().mean()\n",
    "            test_loss.append(loss.item())\n",
    "            test_acc.append(acc.item())\n",
    "    return np.mean(test_loss), np.mean(test_acc)\n",
    "\n",
    "epochs = 10\n",
    "train_loss, train_acc = [], []\n",
    "valid_loss, valid_acc = [], []\n",
    "for epoch in range(1, epochs+1):\n",
    "    print(f'Epoch {epoch}/{epochs}')\n",
    "    loss, acc = train_one_epoch(model, train_loader, loss_function, optimizer, device)\n",
    "    print(f'train_loss - {loss:.4f}, train_acc - {acc:.4f}')\n",
    "    train_loss.append(loss)\n",
    "    train_acc.append(acc)\n",
    "    \n",
    "    loss, acc = test(model, valid_loader, loss_function, device)\n",
    "    print(f'valid_loss - {loss:.4f}, valid_acc - {acc:.4f}')\n",
    "    valid_loss.append(loss)\n",
    "    valid_acc.append(acc)\n",
    "    \n",
    "    print('')\n",
    "    \n",
    "test_loss, test_acc = test(model, test_loader, loss_function, device)\n",
    "print('test_loss = ', test_loss)\n",
    "print('test_acc = ', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルサイズ，入力サイズ，GPUの性能によってこのセルの実行時間は異なるが，Block数を3に制限したとしても，学習にはCNN以上に時間がかかる．またViTは一般的に大規模なデータセットで事前学習を行う必要があり，パラメータを制限したとしても学習はCNNほど上手く進まない．\n",
    "\n",
    "別ノートブックで事前学習済みモデルの利用方法についても紹介したい．\n",
    "\n",
    "続いて，予測結果を出力する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for batch in test_loader:\n",
    "    x, y = batch\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(x)\n",
    "        _, prediction = torch.max(output, 1)\n",
    "\n",
    "    x = x.cpu()\n",
    "    mean_ = torch.tensor(mean.numpy()).view(1, 3, 1, 1)\n",
    "    std_ = torch.tensor(std.numpy()).view(1, 3, 1, 1)\n",
    "    x = x * std_ + mean_\n",
    "    x = x.permute(0,2,3,1)\n",
    "        \n",
    "    fig, axes = plt.subplots(1, 10, figsize=(15, 2))\n",
    "    for i in range(10):\n",
    "        img = x[i].cpu().numpy().squeeze()\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_title(f\"Pred: {prediction[i].item()} - GT: {y[i].item()}\")\n",
    "        axes[i].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの保存\n",
    "\n",
    "モデルを保存する．保存方法はこれまでと同様である．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('output', exist_ok=True)\n",
    "\n",
    "save_path = 'output/model.pth'\n",
    "torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習結果の表示と保存\n",
    "\n",
    "学習曲線の表示と結果の保存もこれまでと同様のコードで実行できる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_loss, label='train_loss')\n",
    "plt.plot(valid_loss, label='valid_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_acc, label='train_acc')\n",
    "plt.plot(valid_acc, label='valid_acc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('output/train_loss.txt', train_loss)\n",
    "np.savetxt('output/train_acc.txt', train_acc)\n",
    "\n",
    "np.savetxt('output/valid_loss.txt', valid_loss)\n",
    "np.savetxt('output/valid_acc.txt', valid_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
