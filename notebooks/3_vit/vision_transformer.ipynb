{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformer\n",
    "\n",
    "**Vision Transformer(ViT)** は自然言語処理で優れた性能を示し，注目を集めた **[Transformer](https://arxiv.org/abs/1706.03762)** の構造をベースにビジョンのために設計されたモデル構造である．ViTの特筆すべき構造は，画像のパッチ埋め込みと自己注意機構（Self-Attention, SA）にある．これらの要素技術は別のノートブックで説明しているので，このノートブックでは **Vision Transformer(ViT)** の全体像の説明と実装を行う．本実装は[timm](https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/vision_transformer.py)のViTの実装を参考にしており，可能な限り，単純化した実装を意識している．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ViTの全体像\n",
    "\n",
    "ViTは，画像のパッチ埋め込みと位置埋め込みを行う入力層，Self-Attentionを含む複数のEncoder Block，クラストークンから予測を行うヘッド（head）から構築される．\n",
    "\n",
    "入力層については別のノートブックで紹介したので，ViTのEncoder Blockを説明する．Encoder Blockは入力トークン `x` に対して，\n",
    "\n",
    "```\n",
    "class Block(nn.Module):\n",
    "    ...\n",
    "    def forward(self, x):\n",
    "        h = self.layer_norm(x)\n",
    "        h = self.attention(h)\n",
    "        h = x + h\n",
    "        h = self.layer_norm(h)\n",
    "        h = self.mlp(h)\n",
    "        h = x + h\n",
    "        return h\n",
    "```\n",
    "\n",
    "と順伝播する．このBlockを複数積み重ねることで深層化する．途中で現れる\n",
    "\n",
    "```\n",
    "h = x + h\n",
    "```\n",
    "\n",
    "は入力をそのまま足し合わせる **残差結合（residual connection）** と呼ばれる仕組みであり，上層からの勾配を減衰させることなく伝播することができる．残差結合は，多層化しても学習が安定する利点がある．\n",
    "\n",
    "これを実装するために，まずは途中で現れるMLPについて説明する．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Block\n",
    "ViTのBlockに含まれるMLPは次のような構造を持つ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim, hidden_dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, dim)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "活性化関数としてReLUではなく **Gaussian Error Linear Unit(GELU)** を用いている．これはReLUを滑らかにした活性化関数である．また正則化としてDropoutを導入している．\n",
    "\n",
    "これらの点を除き，基本的な二層のMLPであることがわかる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Block\n",
    "\n",
    "MLPが定義できたので，次はEncoder Blockを定義する．まずはMulti-Head Self-Attention(MHSA)を定義する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=4, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        self.proj_q = nn.Linear(dim, dim, bias=False)\n",
    "        self.proj_k = nn.Linear(dim, dim, bias=False)\n",
    "        self.proj_v = nn.Linear(dim, dim, bias=False)\n",
    "        self.proj = nn.Linear(dim, dim, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        bs, num_tokens, dim = x.shape\n",
    "        \n",
    "        q = self.proj_q(x)\n",
    "        k = self.proj_q(x)\n",
    "        v = self.proj_q(x)\n",
    "        \n",
    "        q = q.reshape(bs, num_tokens, self.num_heads, self.head_dim)\n",
    "        k = k.reshape(bs, num_tokens, self.num_heads, self.head_dim)\n",
    "        v = v.reshape(bs, num_tokens, self.num_heads, self.head_dim)\n",
    "    \n",
    "        attn_weight = q @ k.transpose(-2, -1) * dim ** -0.5\n",
    "        attn_weight = F.softmax(attn_weight, dim=-1)\n",
    "        attn_weight = self.dropout(attn_weight)\n",
    "        x = attn_weight @ v\n",
    "        \n",
    "        x = x.transpose(1, 2).reshape(bs, num_tokens, dim)\n",
    "        x = self.proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "そして，前述した順伝播になるように次のように定義する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.attn = Attention(dim, num_heads, dropout)\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp = MLP(dim, dim, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.norm1(x)\n",
    "        h = self.attn(h)\n",
    "        h = x + h\n",
    "        h = self.norm2(h)\n",
    "        h = self.mlp(h)\n",
    "        h = x + h\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上より，オリジナルのViTから省略した機能や引数もあるがシンプルなEncoder Blockが構築できた．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Head\n",
    "\n",
    "複数回のBlockを順伝播して得られたクラストークンを入力として受け取り，予測結果を出力するヘッド（head）を作成する．ヘッドはLayer Normと出力次元へ線形変換する線形層から構築される．\n",
    "\n",
    "つまり，次のようになる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fc = nn.Linear(dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上で，ViTの構成要素が定義できた．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "では，ViTのEncoderを定義する．Encoder内部でパッチ化を行う実装が多いので，本実装でも画像を受け取る実装とする．まずは，パッチ埋め込みの定義をする．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=384):\n",
    "        super().__init__()\n",
    "        self.num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
    "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実際のBlock数はもっと多いが，ここでは3つのBlcokを持つViTを定義する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, img_size, patch_size, in_channels, num_classes, embed_dim, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbed(img_size, patch_size, in_channels, embed_dim)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        num_patches = self.patch_embed.num_patches + 1\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n",
    "\n",
    "        self.block1 = Block(embed_dim, num_heads, dropout)\n",
    "        self.block2 = Block(embed_dim, num_heads, dropout)\n",
    "        self.block3 = Block(embed_dim, num_heads, dropout)\n",
    "\n",
    "        self.head = Head(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        cls_tokens = self.cls_token.expand(x.shape[0], -1, -1)\n",
    "        x = torch.torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "\n",
    "        x = self.head(x[:,0])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最後の `x[:,0]` はクラストークンのみをスライシングしてヘッドに入力している．\n",
    "\n",
    "モデルをインスタンス化して，ダミーデータで順伝播の検証をしよう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_x = torch.randn((10, 3, 224, 224))\n",
    "\n",
    "model = ViT(224, 16, 3, 10, 128, 4, 0.5)\n",
    "print(model)\n",
    "\n",
    "y = model(dummy_x)\n",
    "print('y.shape:', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "エラーなく順伝播が実行でき，意図した `(batch_size, num_classes)` の出力を得ることができた．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
